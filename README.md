# Resume-to-DEET Instant Registration System
### *with Automated Job Vacancy Discovery*

> **AI-powered platform** that converts any resume into a completed DEET profile and continuously discovers verified job vacancies from employer career pages.

---

## ğŸš€ Quick Start (Windows)

```bash
# 1. First-time setup (installs everything)
setup.bat

# 2. Start the server
run.bat

# 3. Open in browser
http://localhost:5000
```

**Requirements:** Python 3.10+ â€” [Download here](https://www.python.org/downloads/) *(check "Add to PATH")*

---

## ğŸ“ Project Structure

```
iridescent-cosmic/
â”‚
â”œâ”€â”€ app.py                    # Flask API entry point (all routes)
â”œâ”€â”€ config.py                 # Configuration constants
â”œâ”€â”€ database.py               # SQLite CRUD helpers
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.bat                 # One-click Windows setup
â”œâ”€â”€ run.bat                   # Quick server launcher
â”‚
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ ocr_engine.py         # PDF/image text extraction (pdfplumber + Tesseract)
â”‚   â”œâ”€â”€ nlp_extractor.py      # NLP pipeline (spaCy + regex rules)
â”‚   â”œâ”€â”€ confidence_scorer.py  # Per-field confidence scoring (0.0â€“1.0)
â”‚   â”œâ”€â”€ job_scraper.py        # Career page web scraper (BeautifulSoup)
â”‚   â”œâ”€â”€ job_classifier.py     # ML job category classifier (TF-IDF + LR)
â”‚   â”œâ”€â”€ deduplicator.py       # Cosine similarity duplicate detector
â”‚   â”œâ”€â”€ employer_verifier.py  # Domain trust scorer (DNS + WHOIS)
â”‚   â””â”€â”€ scheduler.py          # APScheduler automated crawl jobs
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html            # Landing page + resume upload
â”‚   â”œâ”€â”€ preview.html          # Editable DEET profile preview
â”‚   â””â”€â”€ jobs.html             # Job discovery board
â”‚
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/style.css         # Dark-mode design system
â”‚   â””â”€â”€ js/
â”‚       â”œâ”€â”€ main.js           # Shared utilities (upload, toast, demo)
â”‚       â”œâ”€â”€ preview.js        # Profile form editor logic
â”‚       â””â”€â”€ jobs.js           # Job board + crawl trigger
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_nlp_extractor.py     # NLP field extraction tests
    â”œâ”€â”€ test_confidence_scorer.py # Confidence scoring tests
    â”œâ”€â”€ test_deduplicator.py      # Duplicate detection tests
    â””â”€â”€ test_api.py               # Flask endpoint integration tests
```

---

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET`  | `/` | Landing page (upload UI) |
| `GET`  | `/jobs` | Job discovery board |
| `GET`  | `/preview` | DEET profile editor |
| `POST` | `/api/resume/upload` | Upload resume â†’ returns extracted profile JSON |
| `POST` | `/api/resume/submit` | Save final profile to database |
| `GET`  | `/api/resume/profiles` | List all saved profiles |
| `GET`  | `/api/jobs/list` | Paginated job listing (filter by category, location, search) |
| `POST` | `/api/jobs/crawl` | Trigger immediate job crawl |
| `POST` | `/api/jobs/add` | Manually add job posting |
| `GET`  | `/api/dashboard/stats` | Platform statistics |
| `GET`  | `/api/health` | Health check |

---

## Module 1 â€” Resume-to-DEET Pipeline

```
Upload (PDF/DOCX/Image)
        â”‚
        â–¼
   OCR Engine (ocr_engine.py)
   â”œâ”€â”€ Native PDF  â†’ pdfplumber
   â”œâ”€â”€ Scanned PDF â†’ pytesseract
   â””â”€â”€ Image       â†’ PIL + pytesseract
        â”‚
        â–¼
   NLP Extractor (nlp_extractor.py)
   â”œâ”€â”€ Section Detection  (regex headers)
   â”œâ”€â”€ NER                (spaCy en_core_web_sm)
   â”œâ”€â”€ Pattern Matching   (email, phone, LinkedIn)
   â”œâ”€â”€ Skills Extraction  (keyword dictionary)
   â””â”€â”€ Date Normalization (dateutil)
        â”‚
        â–¼
   Confidence Scorer (confidence_scorer.py)
   â””â”€â”€ Per-field score 0.0â€“1.0 â†’ HIGH/MEDIUM/LOW color display
        â”‚
        â–¼
   Editable Preview â†’ User reviews â†’ Submit to DB
```

**Extracted fields:** Full Name Â· Email Â· Phone Â· Address Â· LinkedIn Â· GitHub Â· Summary Â· Education Â· Skills Â· Certifications Â· Work Experience Â· Projects

---

## Module 2 â€” Job Vacancy Discovery Pipeline

```
APScheduler (every 6h) or Manual Trigger
        â”‚
        â–¼
   Job Scraper (job_scraper.py)
   â””â”€â”€ requests + BeautifulSoup per career page
        â”‚
        â–¼
   Field Extraction
   â”œâ”€â”€ Experience level (regex patterns)
   â””â”€â”€ Skills required  (keyword matching)
        â”‚
        â–¼
   Deduplicator (deduplicator.py)
   â”œâ”€â”€ Exact: SHA-256 hash of (title+company+location)
   â””â”€â”€ Near-dup: TF-IDF cosine similarity â‰¥ 0.85
        â”‚
        â–¼
   Job Classifier (job_classifier.py)
   â””â”€â”€ TF-IDF + Logistic Regression (fallback: keyword rules)
        â”‚
        â–¼
   Employer Verifier (employer_verifier.py)
   â”œâ”€â”€ DNS resolution  +0.25
   â”œâ”€â”€ HTTP reachable  +0.25
   â”œâ”€â”€ HTTPS           +0.20
   â””â”€â”€ Domain age      +0.30 (â‰¥1 yr) / +0.20 (â‰¥180 days)
        â”‚
        â–¼
   Save to SQLite â†’ Appear on Job Board
```

---

## ğŸ‹ï¸ Running Tests

```bash
# Activate your venv first
.venv\Scripts\activate.bat

# Run all tests
python -m pytest tests/ -v

# Run specific test file
python -m pytest tests/test_nlp_extractor.py -v
python -m pytest tests/test_api.py -v
```

---

## âš™ï¸ Configuration

Edit `config.py` to customise:

| Setting | Default | Description |
|---------|---------|-------------|
| `CRAWL_INTERVAL_HOURS` | `6` | How often to auto-crawl |
| `DEDUP_COSINE_THRESHOLD` | `0.85` | Near-duplicate sensitivity |
| `MIN_DOMAIN_AGE_DAYS` | `180` | Employer trust threshold |
| `TARGET_CAREER_PAGES` | `[example.com]` | Add your target career page URLs |
| `JOB_CATEGORIES` | 10 categories | DEET classification taxonomy |

### Adding Career Pages

In `config.py`, add entries to `TARGET_CAREER_PAGES`:

```python
{
    "company":          "My Company",
    "url":              "https://mycompany.com/careers",
    "job_selector":     ".job-listing",      # CSS selector for each job block
    "title_selector":   ".job-title",
    "location_selector":".job-location",
    "link_selector":    "a",
},
```

---

## ğŸ“Š Evaluation Metrics

| Metric | Target | Method |
|--------|--------|--------|
| Field Extraction F1 | â‰¥ 85% | Compare vs. gold-labeled test resumes |
| OCR Character Error Rate | â‰¤ 5% | Known text vs. OCR output |
| Job Discovery Precision | â‰¥ 90% | Manual audit of scraped jobs |
| Duplicate Detection Rate | â‰¥ 95% | Injected known-duplicate test set |
| Resume API Latency (p95) | â‰¤ 5s | End-to-end timing logs |

---

## ğŸ’¡ Hackathon Innovation Highlights

| Feature | Innovation |
|---------|-----------|
| **Hybrid OCR+NLP** | Falls back from native PDF â†’ scanned OCR â†’ image OCR automatically |
| **Confidence UI** | Color-coded per-field confidence badges guide users to fix weak extractions |
| **Demo Mode** | One-click demo loads a pre-filled profile without needing a real resume |
| **Dual Dedup** | SHA-256 exact + TF-IDF cosine near-duplicate detection |
| **Trust Score** | Multi-factor employer verification (DNS + WHOIS + HTTPS) |
| **Zero-config ML** | Keyword fallback classifier works with zero training data |
| **Auto-scheduler** | Background 6-hour crawl runs without any cron or external service |

---

## ğŸ“¦ Key Dependencies

| Package | Purpose |
|---------|---------|
| `flask` | REST API and HTML templating |
| `pdfplumber` | Native PDF text extraction |
| `pytesseract` | OCR for scanned documents |
| `spacy` | Named Entity Recognition |
| `scikit-learn` | TF-IDF vectorization + job classifier |
| `beautifulsoup4` | HTML parsing for web scraper |
| `APScheduler` | Background job scheduling |
| `python-whois` | Domain age verification |

---

*Built for the DEET Hackathon â€” February 2026*
